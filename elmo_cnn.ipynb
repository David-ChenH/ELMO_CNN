{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/zy/rlk_3zrx72s_z3zjxx3gsv600000gn/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cifar10\n",
    "import cifar10_input\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "batch_size = 128\n",
    "max_steps = 2000\n",
    "\n",
    "# read dataset\n",
    "# # tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# cifar10.maybe_download_and_extract()\n",
    "# batch_dir = './cifar-10-batches-bin'\n",
    "# images_train, labels_train = cifar10_input.distorted_inputs(data_dir=batch_dir, batch_size=batch_size)\n",
    "\n",
    "imgs_train = []\n",
    "for img_num in range(128):\n",
    "    imgs_train.append(cv2.resize(cv2.imread(\"./cats_and_dogs_filtered/train/cats/cat.%d.jpg\" % img_num),(24,24)))\n",
    "\n",
    "labels_train = [\"cat\"]*128\n",
    "                    \n",
    "\n",
    "\n",
    "image_input = tf.placeholder(tf.float32, [None, 24, 24, 3])\n",
    "text_batch = tf.placeholder('string', shape=[None], name='text_input') \n",
    "\n",
    "# define FC network for mapping the output of elmo\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True) #getting ELMo embeddings\n",
    "elmo_embds = elmo(text_batch, signature=\"default\", as_dict=True)\n",
    "w_embd = tf.identity(elmo_embds['word_emb'], name='elmo_word_embd') #?xTxD \n",
    "class_embd = w_embd[:,0,:] #?xD\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal((512, 400), dtype=tf.float32))\n",
    "b1 = tf.Variable(np.random.rand(400), dtype=tf.float32)\n",
    "w2 = tf.Variable(np.random.rand(400, 300), dtype=tf.float32)\n",
    "b2 = tf.Variable(np.random.rand(300), dtype=tf.float32)\n",
    "                 \n",
    "output_text = tf.add(tf.matmul(class_embd, w1), b1)\n",
    "output_text = tf.nn.relu(output_text)\n",
    "output_text = tf.add(tf.matmul(output_text, w2), b2)\n",
    "output_text = tf.nn.relu(output_text)\n",
    "\n",
    "\n",
    "# define image model\n",
    "def generate_weight(shape, stddev, w):\n",
    "    weight = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if w is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(weight), w, name='weight_loss')\n",
    "        tf.add_to_collection('losses',weight_loss)\n",
    "    return weight\n",
    "\n",
    "def generate_bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def conv_2d(x, W):\n",
    "    return tf.nn.conv2d(x,W, strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "\n",
    "# first convolutional layer\n",
    "weight1 = generate_weight(shape=[3, 3, 3, 64],stddev=5e-2, w=0.0)\n",
    "output1 = conv_2d(image_input, weight1)\n",
    "bias1 = generate_bias([64])\n",
    "output1 = tf.nn.relu(tf.add(output1, bias1))\n",
    "output1 = max_pool(output1)\n",
    "output1 = tf.layers.batch_normalization(output1, training=True)\n",
    "\n",
    "#second convolutional layer\n",
    "weight2 = generate_weight(shape=[3, 3, 64, 128],stddev=5e-2, w=0.0)\n",
    "output2 = conv_2d(output1, weight2)\n",
    "bias2 = generate_bias([128])\n",
    "output2 = tf.nn.relu(tf.add(output2, bias2))\n",
    "output2 = max_pool(output2)\n",
    "output2 = tf.layers.batch_normalization(output2, training=True)\n",
    "\n",
    "#third convolutional layer\n",
    "weight3 = generate_weight(shape=[3, 3, 128, 300],stddev=5e-2, w=0.0)\n",
    "output3 = conv_2d(output2, weight3)\n",
    "bias3 = generate_bias([300])\n",
    "output3 = tf.nn.relu(tf.add(output3, bias3))\n",
    "output3 = max_pool(output3)\n",
    "output3 = tf.layers.batch_normalization(output3, training=True)\n",
    "\n",
    "# implement global pool\n",
    "output4 = tf.reduce_mean(output3, axis=[1,2])\n",
    "\n",
    "# loss function\n",
    "def loss(image_output, text_output):\n",
    "    loss_value = tf.losses.mean_squared_error(image_output, text_output)\n",
    "    return tf.add_n(tf.get_collection('losses'), name = 'total_loss')\n",
    "\n",
    "loss_value = loss(output4, output_text)\n",
    "train_opt = tf.train.AdamOptimizer(1e-3).minimize(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss=701236.69(290.9 exaples/sec; 0.440 sec/batch)\n",
      "step 10, loss=410139.25(566.4 exaples/sec; 0.226 sec/batch)\n",
      "step 20, loss=235201.66(543.6 exaples/sec; 0.235 sec/batch)\n",
      "step 30, loss=135001.94(515.9 exaples/sec; 0.248 sec/batch)\n",
      "step 40, loss=81250.17(511.7 exaples/sec; 0.250 sec/batch)\n",
      "step 50, loss=53339.01(495.9 exaples/sec; 0.258 sec/batch)\n",
      "step 60, loss=36527.00(522.0 exaples/sec; 0.245 sec/batch)\n",
      "step 70, loss=25995.29(503.3 exaples/sec; 0.254 sec/batch)\n",
      "step 80, loss=19236.03(501.8 exaples/sec; 0.255 sec/batch)\n",
      "step 90, loss=14454.28(496.5 exaples/sec; 0.258 sec/batch)\n",
      "step 100, loss=10997.41(506.3 exaples/sec; 0.253 sec/batch)\n",
      "step 110, loss=8546.69(491.6 exaples/sec; 0.260 sec/batch)\n",
      "step 120, loss=6772.17(492.1 exaples/sec; 0.260 sec/batch)\n",
      "step 130, loss=5474.82(500.8 exaples/sec; 0.256 sec/batch)\n",
      "step 140, loss=4478.97(469.7 exaples/sec; 0.273 sec/batch)\n",
      "step 150, loss=3691.07(510.0 exaples/sec; 0.251 sec/batch)\n",
      "step 160, loss=3073.75(492.3 exaples/sec; 0.260 sec/batch)\n",
      "step 170, loss=2575.47(474.5 exaples/sec; 0.270 sec/batch)\n",
      "step 180, loss=2179.91(475.7 exaples/sec; 0.269 sec/batch)\n",
      "step 190, loss=1858.33(463.8 exaples/sec; 0.276 sec/batch)\n",
      "step 200, loss=1593.49(496.9 exaples/sec; 0.258 sec/batch)\n",
      "step 210, loss=1396.47(488.3 exaples/sec; 0.262 sec/batch)\n",
      "step 220, loss=1246.06(485.2 exaples/sec; 0.264 sec/batch)\n",
      "step 230, loss=1123.28(483.9 exaples/sec; 0.265 sec/batch)\n",
      "step 240, loss=1011.03(476.2 exaples/sec; 0.269 sec/batch)\n",
      "step 250, loss=912.56(485.2 exaples/sec; 0.264 sec/batch)\n",
      "step 260, loss=825.80(468.9 exaples/sec; 0.273 sec/batch)\n",
      "step 270, loss=750.71(486.1 exaples/sec; 0.263 sec/batch)\n",
      "step 280, loss=681.49(486.3 exaples/sec; 0.263 sec/batch)\n",
      "step 290, loss=617.74(490.4 exaples/sec; 0.261 sec/batch)\n",
      "step 300, loss=564.26(494.7 exaples/sec; 0.259 sec/batch)\n",
      "step 310, loss=518.20(493.3 exaples/sec; 0.259 sec/batch)\n",
      "step 320, loss=477.96(514.4 exaples/sec; 0.249 sec/batch)\n",
      "step 330, loss=445.02(469.9 exaples/sec; 0.272 sec/batch)\n",
      "step 340, loss=414.42(432.1 exaples/sec; 0.296 sec/batch)\n",
      "step 350, loss=385.58(492.8 exaples/sec; 0.260 sec/batch)\n",
      "step 360, loss=358.42(487.8 exaples/sec; 0.262 sec/batch)\n",
      "step 370, loss=332.86(455.2 exaples/sec; 0.281 sec/batch)\n",
      "step 380, loss=308.84(500.4 exaples/sec; 0.256 sec/batch)\n",
      "step 390, loss=288.00(493.0 exaples/sec; 0.260 sec/batch)\n",
      "step 400, loss=269.36(476.7 exaples/sec; 0.269 sec/batch)\n",
      "step 410, loss=254.09(458.4 exaples/sec; 0.279 sec/batch)\n",
      "step 420, loss=241.57(470.1 exaples/sec; 0.272 sec/batch)\n",
      "step 430, loss=229.55(491.6 exaples/sec; 0.260 sec/batch)\n",
      "step 440, loss=218.01(461.7 exaples/sec; 0.277 sec/batch)\n",
      "step 450, loss=206.93(469.8 exaples/sec; 0.272 sec/batch)\n",
      "step 460, loss=196.30(472.3 exaples/sec; 0.271 sec/batch)\n",
      "step 470, loss=186.11(479.5 exaples/sec; 0.267 sec/batch)\n",
      "step 480, loss=176.35(491.8 exaples/sec; 0.260 sec/batch)\n",
      "step 490, loss=167.01(492.2 exaples/sec; 0.260 sec/batch)\n",
      "step 500, loss=158.07(475.9 exaples/sec; 0.269 sec/batch)\n",
      "step 510, loss=149.53(446.8 exaples/sec; 0.286 sec/batch)\n",
      "step 520, loss=141.37(490.5 exaples/sec; 0.261 sec/batch)\n",
      "step 530, loss=133.58(469.8 exaples/sec; 0.272 sec/batch)\n",
      "step 540, loss=126.15(459.2 exaples/sec; 0.279 sec/batch)\n",
      "step 550, loss=119.07(484.2 exaples/sec; 0.264 sec/batch)\n",
      "step 560, loss=112.33(490.6 exaples/sec; 0.261 sec/batch)\n",
      "step 570, loss=105.92(492.7 exaples/sec; 0.260 sec/batch)\n",
      "step 580, loss=99.82(452.5 exaples/sec; 0.283 sec/batch)\n",
      "step 590, loss=94.03(440.2 exaples/sec; 0.291 sec/batch)\n",
      "step 600, loss=88.52(475.5 exaples/sec; 0.269 sec/batch)\n",
      "step 610, loss=83.30(479.4 exaples/sec; 0.267 sec/batch)\n",
      "step 620, loss=78.35(502.4 exaples/sec; 0.255 sec/batch)\n",
      "step 630, loss=73.66(481.4 exaples/sec; 0.266 sec/batch)\n",
      "step 640, loss=69.21(459.6 exaples/sec; 0.279 sec/batch)\n",
      "step 650, loss=65.03(482.0 exaples/sec; 0.266 sec/batch)\n",
      "step 660, loss=61.70(505.7 exaples/sec; 0.253 sec/batch)\n",
      "step 670, loss=58.53(492.0 exaples/sec; 0.260 sec/batch)\n",
      "step 680, loss=55.50(495.5 exaples/sec; 0.258 sec/batch)\n",
      "step 690, loss=52.60(491.5 exaples/sec; 0.260 sec/batch)\n",
      "step 700, loss=49.84(477.4 exaples/sec; 0.268 sec/batch)\n",
      "step 710, loss=47.20(491.6 exaples/sec; 0.260 sec/batch)\n",
      "step 720, loss=44.69(473.9 exaples/sec; 0.270 sec/batch)\n",
      "step 730, loss=42.29(465.0 exaples/sec; 0.275 sec/batch)\n",
      "step 740, loss=40.12(483.1 exaples/sec; 0.265 sec/batch)\n",
      "step 750, loss=38.43(459.6 exaples/sec; 0.278 sec/batch)\n",
      "step 760, loss=36.81(493.6 exaples/sec; 0.259 sec/batch)\n",
      "step 770, loss=35.24(487.5 exaples/sec; 0.263 sec/batch)\n",
      "step 780, loss=33.73(490.4 exaples/sec; 0.261 sec/batch)\n",
      "step 790, loss=32.28(511.4 exaples/sec; 0.250 sec/batch)\n",
      "step 800, loss=30.87(491.2 exaples/sec; 0.261 sec/batch)\n",
      "step 810, loss=29.52(473.0 exaples/sec; 0.271 sec/batch)\n",
      "step 820, loss=28.22(480.0 exaples/sec; 0.267 sec/batch)\n",
      "step 830, loss=26.96(471.8 exaples/sec; 0.271 sec/batch)\n",
      "step 840, loss=25.75(493.1 exaples/sec; 0.260 sec/batch)\n",
      "step 850, loss=24.59(492.5 exaples/sec; 0.260 sec/batch)\n",
      "step 860, loss=23.48(489.6 exaples/sec; 0.261 sec/batch)\n",
      "step 870, loss=22.40(496.3 exaples/sec; 0.258 sec/batch)\n",
      "step 880, loss=21.37(476.6 exaples/sec; 0.269 sec/batch)\n",
      "step 890, loss=20.38(479.7 exaples/sec; 0.267 sec/batch)\n",
      "step 900, loss=19.43(490.1 exaples/sec; 0.261 sec/batch)\n",
      "step 910, loss=18.52(504.7 exaples/sec; 0.254 sec/batch)\n",
      "step 920, loss=17.64(491.7 exaples/sec; 0.260 sec/batch)\n",
      "step 930, loss=16.80(473.7 exaples/sec; 0.270 sec/batch)\n",
      "step 940, loss=16.00(483.9 exaples/sec; 0.265 sec/batch)\n",
      "step 950, loss=15.23(459.3 exaples/sec; 0.279 sec/batch)\n",
      "step 960, loss=14.50(497.6 exaples/sec; 0.257 sec/batch)\n",
      "step 970, loss=13.79(456.8 exaples/sec; 0.280 sec/batch)\n",
      "step 980, loss=13.12(507.5 exaples/sec; 0.252 sec/batch)\n",
      "step 990, loss=12.47(481.3 exaples/sec; 0.266 sec/batch)\n",
      "step 1000, loss=11.85(492.1 exaples/sec; 0.260 sec/batch)\n",
      "step 1010, loss=11.27(490.5 exaples/sec; 0.261 sec/batch)\n",
      "step 1020, loss=10.70(486.5 exaples/sec; 0.263 sec/batch)\n",
      "step 1030, loss=10.17(471.2 exaples/sec; 0.272 sec/batch)\n",
      "step 1040, loss=9.65(479.0 exaples/sec; 0.267 sec/batch)\n",
      "step 1050, loss=9.17(491.0 exaples/sec; 0.261 sec/batch)\n",
      "step 1060, loss=8.70(478.6 exaples/sec; 0.267 sec/batch)\n",
      "step 1070, loss=8.26(479.7 exaples/sec; 0.267 sec/batch)\n",
      "step 1080, loss=7.83(483.6 exaples/sec; 0.265 sec/batch)\n",
      "step 1090, loss=7.43(481.5 exaples/sec; 0.266 sec/batch)\n",
      "step 1100, loss=7.05(449.5 exaples/sec; 0.285 sec/batch)\n",
      "step 1110, loss=6.68(483.8 exaples/sec; 0.265 sec/batch)\n",
      "step 1120, loss=6.34(492.5 exaples/sec; 0.260 sec/batch)\n",
      "step 1130, loss=6.01(484.7 exaples/sec; 0.264 sec/batch)\n",
      "step 1140, loss=5.69(493.5 exaples/sec; 0.259 sec/batch)\n",
      "step 1150, loss=5.43(465.5 exaples/sec; 0.275 sec/batch)\n",
      "step 1160, loss=5.22(486.0 exaples/sec; 0.263 sec/batch)\n",
      "step 1170, loss=5.01(497.8 exaples/sec; 0.257 sec/batch)\n",
      "step 1180, loss=4.82(482.1 exaples/sec; 0.266 sec/batch)\n",
      "step 1190, loss=4.63(474.2 exaples/sec; 0.270 sec/batch)\n",
      "step 1200, loss=4.44(492.7 exaples/sec; 0.260 sec/batch)\n",
      "step 1210, loss=4.26(487.0 exaples/sec; 0.263 sec/batch)\n",
      "step 1220, loss=4.09(499.4 exaples/sec; 0.256 sec/batch)\n",
      "step 1230, loss=3.93(490.0 exaples/sec; 0.261 sec/batch)\n",
      "step 1240, loss=3.77(498.2 exaples/sec; 0.257 sec/batch)\n",
      "step 1250, loss=3.61(494.3 exaples/sec; 0.259 sec/batch)\n",
      "step 1260, loss=3.46(496.0 exaples/sec; 0.258 sec/batch)\n",
      "step 1270, loss=3.32(495.7 exaples/sec; 0.258 sec/batch)\n",
      "step 1280, loss=3.18(493.0 exaples/sec; 0.260 sec/batch)\n",
      "step 1290, loss=3.05(490.8 exaples/sec; 0.261 sec/batch)\n",
      "step 1300, loss=2.92(490.3 exaples/sec; 0.261 sec/batch)\n",
      "step 1310, loss=2.80(491.1 exaples/sec; 0.261 sec/batch)\n",
      "step 1320, loss=2.68(493.7 exaples/sec; 0.259 sec/batch)\n",
      "step 1330, loss=2.57(492.3 exaples/sec; 0.260 sec/batch)\n",
      "step 1340, loss=2.46(491.5 exaples/sec; 0.260 sec/batch)\n",
      "step 1350, loss=2.36(494.0 exaples/sec; 0.259 sec/batch)\n",
      "step 1360, loss=2.26(510.6 exaples/sec; 0.251 sec/batch)\n",
      "step 1370, loss=2.16(480.8 exaples/sec; 0.266 sec/batch)\n",
      "step 1380, loss=2.07(488.4 exaples/sec; 0.262 sec/batch)\n",
      "step 1390, loss=1.98(503.4 exaples/sec; 0.254 sec/batch)\n",
      "step 1400, loss=1.89(486.9 exaples/sec; 0.263 sec/batch)\n",
      "step 1410, loss=1.81(505.4 exaples/sec; 0.253 sec/batch)\n",
      "step 1420, loss=1.74(491.6 exaples/sec; 0.260 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1430, loss=1.66(502.4 exaples/sec; 0.255 sec/batch)\n",
      "step 1440, loss=1.59(472.4 exaples/sec; 0.271 sec/batch)\n",
      "step 1450, loss=1.52(497.8 exaples/sec; 0.257 sec/batch)\n",
      "step 1460, loss=1.45(485.3 exaples/sec; 0.264 sec/batch)\n",
      "step 1470, loss=1.39(486.7 exaples/sec; 0.263 sec/batch)\n",
      "step 1480, loss=1.33(454.2 exaples/sec; 0.282 sec/batch)\n",
      "step 1490, loss=1.28(498.4 exaples/sec; 0.257 sec/batch)\n",
      "step 1500, loss=1.24(493.9 exaples/sec; 0.259 sec/batch)\n",
      "step 1510, loss=1.21(492.4 exaples/sec; 0.260 sec/batch)\n",
      "step 1520, loss=1.17(446.7 exaples/sec; 0.287 sec/batch)\n",
      "step 1530, loss=1.14(462.8 exaples/sec; 0.277 sec/batch)\n",
      "step 1540, loss=1.10(488.7 exaples/sec; 0.262 sec/batch)\n",
      "step 1550, loss=1.07(502.7 exaples/sec; 0.255 sec/batch)\n",
      "step 1560, loss=1.04(501.2 exaples/sec; 0.255 sec/batch)\n",
      "step 1570, loss=1.01(494.7 exaples/sec; 0.259 sec/batch)\n",
      "step 1580, loss=0.98(475.7 exaples/sec; 0.269 sec/batch)\n",
      "step 1590, loss=0.95(475.9 exaples/sec; 0.269 sec/batch)\n",
      "step 1600, loss=0.92(486.2 exaples/sec; 0.263 sec/batch)\n",
      "step 1610, loss=0.89(466.1 exaples/sec; 0.275 sec/batch)\n",
      "step 1620, loss=0.86(487.5 exaples/sec; 0.263 sec/batch)\n",
      "step 1630, loss=0.84(467.9 exaples/sec; 0.274 sec/batch)\n",
      "step 1640, loss=0.81(487.3 exaples/sec; 0.263 sec/batch)\n",
      "step 1650, loss=0.79(481.3 exaples/sec; 0.266 sec/batch)\n",
      "step 1660, loss=0.76(492.6 exaples/sec; 0.260 sec/batch)\n",
      "step 1670, loss=0.74(491.1 exaples/sec; 0.261 sec/batch)\n",
      "step 1680, loss=0.71(496.4 exaples/sec; 0.258 sec/batch)\n",
      "step 1690, loss=0.69(491.8 exaples/sec; 0.260 sec/batch)\n",
      "step 1700, loss=0.67(495.3 exaples/sec; 0.258 sec/batch)\n",
      "step 1710, loss=0.65(481.5 exaples/sec; 0.266 sec/batch)\n",
      "step 1720, loss=0.63(509.6 exaples/sec; 0.251 sec/batch)\n",
      "step 1730, loss=0.61(495.5 exaples/sec; 0.258 sec/batch)\n",
      "step 1740, loss=0.59(485.9 exaples/sec; 0.263 sec/batch)\n",
      "step 1750, loss=0.57(492.7 exaples/sec; 0.260 sec/batch)\n",
      "step 1760, loss=0.55(492.0 exaples/sec; 0.260 sec/batch)\n",
      "step 1770, loss=0.54(491.9 exaples/sec; 0.260 sec/batch)\n",
      "step 1780, loss=0.52(442.0 exaples/sec; 0.290 sec/batch)\n",
      "step 1790, loss=0.50(486.3 exaples/sec; 0.263 sec/batch)\n",
      "step 1800, loss=0.49(483.2 exaples/sec; 0.265 sec/batch)\n",
      "step 1810, loss=0.47(479.0 exaples/sec; 0.267 sec/batch)\n",
      "step 1820, loss=0.46(447.0 exaples/sec; 0.286 sec/batch)\n",
      "step 1830, loss=0.44(470.4 exaples/sec; 0.272 sec/batch)\n",
      "step 1840, loss=0.43(497.1 exaples/sec; 0.257 sec/batch)\n",
      "step 1850, loss=0.42(491.3 exaples/sec; 0.261 sec/batch)\n",
      "step 1860, loss=0.41(492.5 exaples/sec; 0.260 sec/batch)\n",
      "step 1870, loss=0.39(512.6 exaples/sec; 0.250 sec/batch)\n",
      "step 1880, loss=0.38(489.1 exaples/sec; 0.262 sec/batch)\n",
      "step 1890, loss=0.37(492.4 exaples/sec; 0.260 sec/batch)\n",
      "step 1900, loss=0.36(495.5 exaples/sec; 0.258 sec/batch)\n",
      "step 1910, loss=0.35(474.4 exaples/sec; 0.270 sec/batch)\n",
      "step 1920, loss=0.34(512.0 exaples/sec; 0.250 sec/batch)\n",
      "step 1930, loss=0.33(492.1 exaples/sec; 0.260 sec/batch)\n",
      "step 1940, loss=0.32(492.0 exaples/sec; 0.260 sec/batch)\n",
      "step 1950, loss=0.31(466.4 exaples/sec; 0.274 sec/batch)\n",
      "step 1960, loss=0.30(464.9 exaples/sec; 0.275 sec/batch)\n",
      "step 1970, loss=0.29(458.4 exaples/sec; 0.279 sec/batch)\n",
      "step 1980, loss=0.28(474.8 exaples/sec; 0.270 sec/batch)\n",
      "step 1990, loss=0.27(497.9 exaples/sec; 0.257 sec/batch)\n",
      "Model saved in path: ./model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        free, loss_value1 = sess.run([train_opt, loss_value], feed_dict = {image_input: imgs_train, text_batch: labels_train})\n",
    "        duration = time.time() - start_time \n",
    "        if step % 10 == 0:\n",
    "            example_per_sec = batch_size / duration\n",
    "            sec_per_batch = float(duration) \n",
    "            format_str = ('step %d, loss=%.2f(%.1f exaples/sec; %.3f sec/batch)')\n",
    "            print(format_str % (step, loss_value1, example_per_sec, sec_per_batch))\n",
    "    \n",
    "    save_path = saver.save(sess, \"./model/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n",
      "The loss of cat image and label 'cat' is: 0.265908\n",
      "The loss of cat image and label 'dog' is: 326622.843750\n"
     ]
    }
   ],
   "source": [
    "test_cat_img = cv2.resize(cv2.imread(\"./cats_and_dogs_filtered/train/cats/cat.129.jpg\"), (24,24))\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/model.ckpt\")\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    a = sess.run(loss_value, feed_dict = {text_batch: [\"cat\"], image_input: [test_cat_img]})\n",
    "    b = sess.run(loss_value, feed_dict = {text_batch: [\"dog\"], image_input: [test_cat_img]})\n",
    "    print(\"The loss of cat image and label 'cat' is: %f\" % a)\n",
    "    print(\"The loss of cat image and label 'dog' is: %f\" % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
